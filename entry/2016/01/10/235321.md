---
Title: パフォーマンス解析とチューニングの方法論
Date: 2016-01-10T23:53:21+09:00
URL: http://memo.yuuk.io/entry/2016/01/10/235321
EditURL: https://blog.hatena.ne.jp/y_uuki/performance.hatenablog.jp/atom/entry/6653586347152488511
Draft: true
---

[Systems Performance](http://www.amazon.co.jp/exec/obidos/ASIN/0133390098/yuuki0b2-22/) Chapter 2.5 Methodlogies.

[:contents]

### Streetlight anti-method
[streetlight effect](https://en.wikipedia.org/wiki/Streetlight_effect) と言われるように、探しやすいところから探すのはよくない。パフォーマンスに置き換えると、他のツールの読み方や使い方を知らないために、とりあえずtopでみようとするなど。

> アプリケーションエンジニアがとりあえずtopを開いてたり、モニタリングツールを開いたりするけど、読み方を知らないというのは結構ある

### Random change anti-method

チューニングパラメータを適当に変えて、結果をみて、よくなるまでまた適当に変えてみるような方法論。
たいてい時間の無駄である。何が起きるのか理解していない変更は負荷がピークの間に問題を引き起こす。

> ISUCON でもこの方法論は時間の無駄でだいたいうまくいかない。

### Blame-someone-else anti-method

自分でパフォーマンスイシューを調査せずに、他のチームに投げてしまう。本当にそのチームの問題であってもなくても、そのチームのリソースが無駄になる。
仮設を引き出すためのデータが不足しているときにこの方法論に陥りうる。

これを避けるためには、問題の報告者に、どのツールを使ったのか、そのツールの出力をどのように解釈してたのかをスクリーンショット付きで尋ねる。

> 普段よくやってる。

### Ad hoc checklist method
サポートのプロフェッショナルが使う方法論。

```
- [ ] iostat –x 1
- [ ] awaitカラムをチェック
- [ ] 定常的に 10 (ms)を超えたら、そのディスクは遅いか過負荷になっている。
```

のようなシステムをチェックするもしくはチューニングするためのチェックリストを整備しておく。

> 普段あまり整備されていなくて、過去のイシューの調査結果から、今問題になっているイシューに適用可能かを適宜判断するようになっている。
サポートタスクに限らず、どうしても自動化できない運用手順というのは存在するので、手順書整備は最近関心が高い。

### Problem statement

- 1. どうしてあなたはパフォーマンスが問題だと思ったんですか？
- 2. このシステムはこれまでうまく動いていましたか？
- 3. 最近、何か変更をしましたか？ソフトウェア？ハードウェア？負荷？
- 4. その問題をレイテンシかランタイムの観点で表現できますか？
- 5. その問題は他の人やアプリケーションに影響していますか？（あなただけの問題ですか？）
- 6. 環境は何ですか？ソフトウェアとハードウェアは何を使っていますか？バージョン？設定？

これらの質問をして回答してもらうだけで、原因と解決がわかることがよくある。

> たしかに問題があればこういうことを考えたり、質問したりしている。自分自身がイシューに取り組むときにも自問自答するとよさそう。

### Scientific method
仮設と実証を繰り返して、未知に迫る科学的手法。

- 1. Question（疑問）
  -  パフォーマンスについては前述のProblem statementに相当
- 2. Hypothesis（仮設）
- 3. Prediction（予測）
- 4. Test（実証）
- 5. Analysis（解析）

Observationalな例。

- 1. Question: スロークエリを引き起こしてるのは何か？
- 2. Hypothesis: クラウド環境で同居してる仮想ホストが(ファイルシステム経由で)ディスクI/Oを食いつぶしているのではないか。
- 3. Prediction: ファイルシステムのI/Oレイテンシを計測すれば、ファイルシステムがスロークエリの原因であることがわかる。
- 4. Test: クエリのレイテンシのうち、データベースファイルシステムのレイテンシの割合をトレースすると、ファイルシステムのレイテンシは5%以下だった。
- 5. Analysis: ファイルシステムとディスクはスロークエリの原因ではない。

Experimentalな例。

- 1. Question: なぜ、ホストBからホストCよりもホストAからホストCのほうがHTTPリクエストに時間がかかるのか。 Why do HTTP requests take longer from host A to host C than from host B to host C?
- 2. Hypothesis: ホストAとホストBは異なるデータセンターにあるからではないか。
- 3. Prediction: ホストAをホストBと同じデータセンターにおけば問題は解決するだろう。
- 4. Test: ホストAを移動して計測した。
- 5. Analysis: 問題は解決した。仮設通りになった。

>
わりと当たり前にやってる気もするけど、ちゃんと方法論を意識していてやってはいない。
GitHubのissueをこういうフォーマットにしてみてもよさそう。

### Diagnosis cycle

Scientific methodに似ている。

```
hypothesis ->  instrumentation ->  data -> hypothesis
```

このサイクルはdataがすぐに新しい仮説を引き出すところが強調されている。
小さなテストを繰り返して仮説をリファインしていく。

>
確かに前述のScientific methodに丁寧に従うと、時間がかかりすぎることもある気がする。
いきなりホストAを移動させて、うまくいかなかったら戻したりするのは手間がありそうなので、まずホストBと同じDC内の他のホストでレスポンスタイムを計測してみるとかして小さなテストを試すのはよさそう。

### Tools method

- 1. 利用可能なパフォーマンスツールを列挙する。
- 2. 各ツールは有用なメトリックを提供してくれる。
- 3. 各メトリックを解釈するための規則を列挙する。

最終的には、チェックリストになる。そこそこ有用だが、利用可能なツールに依存してしまって、システムの正しい状況がわからないこともある。

> あんまり使わないほうがよさそうにみえる。

### USE method

USE=tilization, saturation, and errors

```
For every resource, check utilization, saturation, and errors.
```

- Resource: 物理サーバのすべてのコンポーネント（CPU、バスなど）
- Utilization: 任意のインターバルのうち、リソースがビジーだった時間の割合。 ビジーだが、リソースはまだ負荷を受けいられるときもある。負荷をさばけられない度合いをsaturationで表す。
- Saturation: リソースが余分な負荷をさばける度合い。
- Errors: エラーイベントの個数

メインメモリなどのcapacity-basedのリソースは、一旦100% utilizationになったら、もうサービスできない。キューに貯まる(Saturation)かエラーになる。これは time-basedなリソースの定義とは異なる。

tools methodに対して、USE methodはツールの代わりにシステムリソースに対してイテレーションする。たとえ、あるリソースのSaturationがチェックできなくても、そのリソースのSaturationはチェックできないという知見は蓄積されるのがUSEメソッドのよいところ。

> 著者であるGreggが提案する方法論。これも普段意識せずにやっているような気はする。time-based Utilizationが100%になっても、まだ負荷をさばける状態が具体的にどのようなものかをまだわかっていない。

USE Methodについて詳しくは (TODO)。

### Workload characterization

結果となるパフォーマンスより、インプットに着目する方法論。

- (Who) 誰がその負荷を引き起こしているのか？プロセスID？ユーザID？リモートIPアドレス？
- (Why) なぜその負荷が起きているのか？コードパス？スタックツリー？
- (What) 負荷の性質は何か？IOPS、スループット、read or write? 平常時との差異も含む。
- (How) どのようにして負荷は変化しているのか？一日単位の変化のパターンはあるか？

余分な負荷を消す以上のパフォーマンス改善はない。Workload characterization methodはこのような改善に有用である。

ワークロードの解析は、アーキテクチャの問題と負荷の問題を分離できる。

Workload characterizationのためのツールやメトリックは解析対象に依存する。

> アクセスログの解析などはこれに相当する。確かに、その負荷はどこからやってきたのかとかいつも考えて障害対応してる気がする。

### Drill-down analysis

```
1. Monitoring: This is used for continually recording high-level statistics over time, and identifying or alerting if a problem may be present.
2. Identification: Given a suspected problem, this narrows the investigation to particular resources or areas of interest, identifying possible bottlenecks.
3. Analysis: Further examination of particular system areas is done to attempt to root-cause and quantify the issue.
```

> NewRelicなんかはこのメソッドを実現する機能に強い。

### Latency analysis

drill-down analysisに似ており、latency analysisは

### Method R
### Event tracing
### Baseline statistics
### Static performance tuning
### Cache tuning
### Micro-benchmarking

### Performance monitoring
TODO
### Queueing theory
TODO
### Capacity planning
TODO
